---
title: Chapter 4 Problems and Notes
output: html_document
---

```{r setup}
library(tidyverse)
library(rethinking)
library(gt)

# Load Howell !Kung data from rethinking package
data(Howell1)
```

## Notes

### Example normal model of height (sec. 4.3)
```{r example_4.3}
# Limit Howell data to adults
d2 <- Howell1[Howell1$age >= 18, ]

# Height model
mu_list <- seq(from = 150, to = 160, length.out = 100)
sigma_list <- seq(from = 7, to = 9, length.out = 100)
post <- expand.grid(mu = mu_list, sigma = sigma_list)
post$LL <- sapply(1:nrow(post), 
                  function(i) 
                    sum(dnorm(d2$height, post$mu[[i]], post$sigma[[i]], log = TRUE)))
post$prod <- post$LL +
  dnorm(post$mu, 178, 20, log = TRUE) +
  dunif(post$sigma, 0, 50, log = TRUE)
post$prob <- exp(post$prod - max(post$prod))

# Sample from posterior
sample_rows <- sample(1:nrow(post) - 1, size = 1e4, replace = TRUE, prob = post$prob)
sample_mu_sigma <- post[sample_rows, c("mu", "sigma")]
plot(sample_mu_sigma, cex = 0.5, pch = 16, col = col.alpha(rangi2, 0.1))

```

### Example linear regression (p. 97-110)
```{r example_4.4}
# Scatter plot of height and weight
plot(d2$height, d2$weight)

# Define mean of weight for centering
xbar <- mean(d2$weight)

m4.3 <- quap(
    alist(
        height ~ dnorm(mu, sigma),
        mu <- a + b * (weight - xbar),
        a ~ dnorm(178, 20),
        b ~ dlnorm(0, 1),
        sigma ~ dunif(0, 50)
    ),
    data = d2
)

# Summary of posterior
precis(m4.3)

# Sample from the posterior
post <- extract.samples(m4.3)
# Mean of posterior a and b parameters
a_map <- mean(post$a)
b_map <- mean(post$b)
ggplot(data = d2, aes(x = weight, y = height)) +
  geom_point(color = rangi2, shape = "o", size = 4) +
  geom_abline(slope = b_map, intercept = a_map - b_map * xbar, lwd = 1) +
  geom_abline(data = post[1:20, ],
              aes(slope = b, intercept = a - b * xbar),
              alpha = 0.3) +
  theme_minimal()

# Distribution of mu at weight values
weight_seq <- 25:70
mu <- link(m4.3, data = data.frame(weight = weight_seq))
mu_mean <- apply(mu, 2, mean)
mu_pi <- apply(mu, 2, PI, prob = 0.89)
# Reshape `mu_pi` for easier plotting
mu_pi <- t(mu_pi) %>%
  as_tibble() %>%
  rename_with(~{paste("mu", .x, sep = "_")}) %>%
  mutate(weight = weight_seq)
# Generate posterior predictions
sim_height <- sim(m4.3, data = list(weight = weight_seq), n = 1e4)
height_pi <- apply(sim_height, 2, PI, prob = 0.89) %>%
  t() %>%
  as_tibble() %>%
  rename_with(~{paste("height", .x, sep = "_")}) %>%
  mutate(weight = weight_seq)


ggplot() +
  geom_point(data = d2,
             aes(x = weight, y = height),
             color = rangi2,
             shape = "o",
             size = 4) +
  geom_abline(slope = b_map, intercept = a_map - b_map * xbar) +
  geom_ribbon(data = mu_pi, aes(x = weight, ymin = `mu_5%`, ymax = `mu_94%`),
              alpha = 0.4,
              fill = "gray") +
  geom_ribbon(data = height_pi,
              aes(x = weight, ymin = `height_5%`, ymax = `height_94%`),
              alpha = 0.3) +
  theme_minimal()
```

## Problems
**4E1.** The first line $y_i \sim \text{Normal}(\mu,\sigma)$ is the likelihood.

**4E2.** There are two paramaters in the posterior distribution $\mu$ and $\sigma$.

**4E3.** Bayes' Theorem says
$$
\begin{align*}
\text{Pr}(\mu,\sigma|y_i)&\propto\text{Pr}(y_i|\mu,\sigma)\text{Pr}(\mu)\text{Pr}(\sigma) \\
&=\frac{\text{Normal}(y_i|\mu,\sigma)\text{Normal}(\mu|0,10)\text{Exponential}(\sigma|1)}{\int\text{Normal}(y_i|\mu,\sigma)\text{Normal}(\mu|0,10)\text{Exponential}(\sigma|1)\text d\mu\text d\sigma}
\end{align*}
$$

**4E4.** The linear model is the second line $\mu_i=\alpha + \beta x_i$.

**4E5.** There are three parameters in the posterior distribution: $\alpha$, $\beta$, and $\sigma$.

**4M1.** 
```{r p4m1}
prior_mu <- rnorm(1e4, mean = 0, sd = 10)
prior_s <- rexp(1e4, rate = 1)
y_prior_pred <- rnorm(1e4, mean = prior_mu, s = prior_s)
hist(y_prior_pred)
```

**4M2.**
```{r p4m2, eval=FALSE}
quap(
    alist(
        y ~ dnorm(mu, sigma),
        mu ~ dnorm(0, 10),
        sigma ~ dexp(1)
    )
)
```

**4M3.**
$$
\begin{align*}
y_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta x_i \\
\alpha &\sim \text{Normal}(0, 10) \\
\beta &\sim \text{Uniform}(0, 1) \\
\sigma &\sim \text{Exponential}(1)
\end{align*}
$$

**4M4.**

A model for this problem could be:

$$
\begin{align*}
y_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta x_i \\
\alpha &\sim \text{Normal}(168, 20) \\
\beta &\sim \text{Exponential}(0.5) \\
\sigma &\sim \text{Exponential}(1)
\end{align*}
$$
where $x_i$ is the year of the study (1, 2, or 3) minus 1. 

* $\alpha$ is the average height in the first year of the study. It's prior has an average of roughly 5'5" and a standard deviation of 20 cm. 
* Since we are dealing with students, we may assume that they don't shrink during the study. So, I chose the Exponential distribution with a mean of 2 cm.
* The variance parameter was chosen as recommended in the chapter.

**4M5.**

No, this doesn't change my priors or model as the fact that students get taller is guaranteed by my prior for $\beta$ already.

**4M6.**

Knowing that the variance of heights among students of the same age was never more than 64 cm would make me change my prior on $\sigma$ to $\text{Uniform}(0,64)$. This would allow for the variance to be positive while encoding the upper-bound to be no more than 64.

**4M7.**
```{r p4m7}
# Fit model 4.3 without centered weights
d2 <- Howell1[Howell1$age >= 18, ]
m4.3_alt <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * weight,
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d2
)

# Summary of posterior
precis(m4.3_alt)
vcov(m4.3_alt)

# Sample from the posterior
post_alt <- extract.samples(m4.3_alt)
# Mean of posterior a and b parameters
a_map <- mean(post_alt$a)
b_map <- mean(post_alt$b)
ggplot(data = d2, aes(x = weight, y = height)) +
  geom_point(color = rangi2, shape = "o", size = 4) +
  geom_abline(slope = b_map, intercept = a_map - b_map, lwd = 1) +
  geom_abline(data = post_alt[1:20, ],
              aes(slope = b, intercept = a - b),
              alpha = 0.3) +
  theme_minimal()

# Distribution of mu at weight values
weight_seq <- 25:70
mu <- link(m4.3_alt, data = data.frame(weight = weight_seq))
mu_mean <- apply(mu, 2, mean)
mu_pi <- apply(mu, 2, PI, prob = 0.89)
# Reshape `mu_pi` for easier plotting
mu_pi <- t(mu_pi) %>%
  as_tibble() %>%
  rename_with(~{paste("mu", .x, sep = "_")}) %>%
  mutate(weight = weight_seq)
# Generate posterior predictions
sim_height <- sim(m4.3_alt, data = list(weight = weight_seq), n = 1e4)
height_pi <- apply(sim_height, 2, PI, prob = 0.89) %>%
  t() %>%
  as_tibble() %>%
  rename_with(~{paste("height", .x, sep = "_")}) %>%
  mutate(weight = weight_seq)


ggplot() +
  geom_point(data = d2,
             aes(x = weight, y = height),
             color = rangi2,
             shape = "o",
             size = 4) +
  geom_abline(slope = b_map, intercept = a_map) +
  geom_ribbon(data = mu_pi, aes(x = weight, ymin = `mu_5%`, ymax = `mu_94%`),
              alpha = 0.4,
              fill = "gray") +
  geom_ribbon(data = height_pi,
              aes(x = weight, ymin = `height_5%`, ymax = `height_94%`),
              alpha = 0.3) +
  theme_minimal()
```

Observations:

- The estimates for $\sigma$ don't change; the estimates for $\beta$ are very similar; but, the estimate for $\alpha$ changes: the mean is lower (154.60 to 114.53), but the standard deviation is greater (0.27 vs. 1.90)
- The parameters now have some covariance, which they didn't have when the weights were centered. However, the covariance is relatively small: the largest (in abs. value) is between $\text{Cov}(\alpha, \beta)$ = `r vcov(m4.3_alt)[1, 2]`.
- The predictions of the centered and non-centered models are similar.

**4M8.**
```{r p4m8}
# Load cherry blossoms data from rethinking
data("cherry_blossoms")
# Remove years with missing data
cb <- cherry_blossoms[complete.cases(cherry_blossoms$doy), ]

# Define function to vary output by number of knots and spread of prior on w
vary_inputs <- function(num_knots, w_spread = 10) {
  knot_list <- quantile(cb$year, probs = seq(0, 1, length.out = num_knots))
  
  # Define splines
  B <- splines::bs(cb$year, knots = knot_list[-c(1, num_knots)], degree = 3, 
                   intercept = TRUE)
  # Fit the model
  cb_m1 <- quap(
    alist(
      D ~ dnorm(mu, sigma),
      mu <- a + B %*% w,
      a ~ dnorm(100, 10),
      w ~ dnorm(0, w_spread),
      sigma ~ dexp(1)
    ),
    data = list(D = cb$doy, B = B, w_spread = w_spread),
    start = list(w = rep(0, ncol(B)))
  )
  
  # Extract samples from the posterior and calculate $\mu$ and its 97% interval
  post <- extract.samples(cb_m1)
  w <- apply(post$w, 2, mean)
  mu <- link(cb_m1)
  mu_PI <- apply(mu, 2, PI, 0.97)
  
  # Plots
  plot_basis <- B |>
    as_tibble(.name_repair = ~paste0("B", .x)) |>
    bind_cols("year" = cb$year) |>
    pivot_longer(cols = -year) |>
    ggplot() +
    geom_line(aes(x = year, y = value, group = name)) +
    scale_y_continuous(breaks = c(0, 1), name = "basis value") +
    scale_x_continuous(breaks = seq(800, 2000, by = 200)) +
    geom_text(data = tibble(x = knot_list, y = 1.05, label = "+"),
              aes(x = x, y = y, label = label),
              size = 6) + 
    theme_minimal()
  
  plot_wtd_basis <- B %*% diag(w) |>
    as_tibble() |>
    bind_cols("year" = cb$year) |>
    pivot_longer(cols = -year) |>
    ggplot() +
    geom_line(aes(x = year, y = value, group = name)) +
    scale_y_continuous(breaks = c(0), name = "basis * weight") +
    scale_x_continuous(breaks = seq(800, 2000, by = 200)) +
    geom_text(data = tibble(x = knot_list, y = 6.05, label = "+"),
              aes(x = x, y = y, label = label),
              size = 6) +
    theme_minimal()
  
  mu_pi <- t(mu_PI) |>
    as_tibble() |>
    bind_cols("year" = cb$year)
  plot_mu <- ggplot() +
    geom_point(data = cb,
               aes(x = year, y = doy),
               color = rangi2,
               alpha = 0.8,
               pch = 16) +
    geom_ribbon(data = mu_pi,
                aes(x = year, ymin = `2%`, ymax = `98%`),
                alpha = 0.5) +
    scale_x_continuous(breaks = seq(800, 2000, by = 200),
                       name = "year") +
    labs(y = "Day in year") +
    theme_minimal()
  
  return(gridExtra::grid.arrange(plot_basis, plot_wtd_basis, plot_mu, nrow = 3))
}
vary_inputs(40, 25)
```

**4H1.**
```{r p4h1}
# Define the new weights
new_weight <- c(46.95, 43.72, 64.78, 32.59, 54.63)
# Simulate posterior predictions for new weights
new_height <- sim(m4.3, data = list(weight = new_weight))
new_height_pi <- apply(new_height, 2, PI, prob = 0.89)
new_height_pi |>
  as_tibble(.name_repair = "unique") |>
  summarize(across(everything(),
            list(mean = mean, lower = min, upper = max))) |>
  pivot_longer(everything()) |>
  separate(name, into = c("var", "measure"), sep = "_") |>
  pivot_wider(id_cols = var, names_from = measure) |>
  mutate(weight = new_weight) |>
  select(individual = var, weight, mean, lower, upper) |>
  gt(rowname_col = "individual") |>
    tab_stubhead(label = "Individual") |>
    cols_label(
      weight = "weight",
      mean = "expected height",
      lower = "lower bound",
      upper = "upper bound"
    ) |>
  tab_spanner(
    label = "89% interval",
    columns = c(lower, upper)
  )
```

**4H2.**

(a) Fitting a linear regression to the Howell data with ages below 18
```{r p4h2a}
d3 <- Howell1[Howell1$age < 18, ]
# Scatter plot of height and weight
plot(d3$weight, d3$height)

# Define mean of weight for centering
xbar <- mean(d3$weight)

m_l_u18 <- quap(
    alist(
        height ~ dnorm(mu, sigma),
        mu <- a + b * (weight - xbar),
        a ~ dnorm(120, 20),
        b ~ dlnorm(0, 1),
        sigma ~ dunif(0, 50)
    ),
    data = d3
)

# Summary of posterior
precis(m_l_u18)
```
The model predicts that for every 10 units of increase in weight, the child will be about 27cm taller.

(b) Plotting the raw data and model summaries

```{r ph2b}
# Sample from the posterior
post <- extract.samples(m_l_u18)
# Mean of posterior a and b parameters
a_map <- mean(post$a)
b_map <- mean(post$b)
# Distribution of mu at weight values
weight_seq <- d3$weight
mu <- link(m_l_u18, data = data.frame(weight = weight_seq))
mu_mean <- apply(mu, 2, mean)
mu_pi <- apply(mu, 2, PI, prob = 0.89)
# Reshape `mu_pi` for easier plotting
mu_pi <- t(mu_pi) %>%
  as_tibble() %>%
  rename_with(~{paste("mu", .x, sep = "_")}) %>%
  mutate(weight = weight_seq)
# Generate posterior predictions
sim_height <- sim(m_l_u18, data = list(weight = weight_seq), n = 1e4)
height_pi <- apply(sim_height, 2, PI, prob = 0.89) %>%
  t() %>%
  as_tibble() %>%
  rename_with(~{paste("height", .x, sep = "_")}) %>%
  mutate(weight = weight_seq)


ggplot() +
  geom_point(data = d3,
             aes(x = weight, y = height),
             color = rangi2,
             shape = "o",
             size = 4) +
  geom_abline(slope = b_map, intercept = a_map - b_map * xbar) +
  geom_ribbon(data = mu_pi, aes(x = weight, ymin = `mu_5%`, ymax = `mu_94%`),
              alpha = 0.4,
              fill = "gray") +
  geom_ribbon(data = height_pi,
              aes(x = weight, ymin = `height_5%`, ymax = `height_94%`),
              alpha = 0.3) +
  theme_minimal()
```

(c) The model does not fit very well at the lower and upper ends of the weight values. The reason is that the data appear to be non-linear. One could try a transformation of weight, say the logarithm or a root, so that the relationship with height is closer to linear.

**4H3.**

(a) Log-linear model for height as a function of weight with all the Howell data
```{r p4h3a}
m_howell <- quap(
    alist(
        height ~ dnorm(mu, sigma),
        mu <- a + b * log(weight),
        a ~ dnorm(100, 20),
        b ~ dlnorm(0, 1),
        sigma ~ dunif(0, 50)
    ),
    data = Howell1
)

# Summary of posterior
precis(m_howell)

# Sample from the posterior
post <- extract.samples(m_howell)
# Mean of posterior a and b parameters
a_map <- mean(post$a)
b_map <- mean(post$b)
# Distribution of mu at weight values
weight_seq <- Howell1$weight
mu <- link(m_howell, data = data.frame(weight = weight_seq))
mu_mean <- apply(mu, 2, mean)
mu_pi <- apply(mu, 2, PI, prob = 0.97)
# Reshape `mu_pi` for easier plotting
mu_pi <- t(mu_pi) %>%
  as_tibble() %>%
  rename_with(~{paste("mu", .x, sep = "_")}) %>%
  mutate(weight = weight_seq)
# Generate posterior predictions
sim_height <- sim(m_howell, data = list(weight = weight_seq), n = 1e4)
height_pi <- apply(sim_height, 2, PI, prob = 0.97) %>%
  t() %>%
  as_tibble() %>%
  rename_with(~{paste("height", .x, sep = "_")}) %>%
  mutate(weight = weight_seq)
```

The parameter estimates of the model are difficult to interpret due to the `log(weight)` transformation. For $b$ we can say that $p$% increase in weight would cause the model to predict (on average) a $b\log(1+p/100)$ increase in height. This is a bit awkward, but for $a% it's worse. One way to say it would be that it is the average height (predicted by the model) when the weight is 1. But, $a$ is negative, which doesn't make sense. But, as far as I know, there are no post-birth humans that weight one kg. 

(b) Plotting model estimates

```{r p4h3b}
mean_function <- function(xvar) {
  a_map + b_map * log(xvar)
}
ggplot() +
  geom_point(data = Howell1,
             aes(x = weight, y = height),
             color = rangi2,
             shape = "o",
             size = 4) +
  stat_function(fun = mean_function) +
  geom_ribbon(data = mu_pi, aes(x = weight, ymin = `mu_2%`, ymax = `mu_98%`),
              alpha = 0.4,
              fill = "gray") +
  geom_ribbon(data = height_pi,
              aes(x = weight, ymin = `height_2%`, ymax = `height_98%`),
              alpha = 0.3) +
  theme_minimal()
```

**4H4.**

---
title: Chapter 4 Problems and Notes
output: html_document
---

```{r setup}
library(tidyverse)
library(rethinking)
library(gt)

# Load Howell !Kung data from rethinking package
data(Howell1)
```

## Notes

### Example normal model of height (sec. 4.3)
```{r example_4.3}
# Limit Howell data to adults
d2 <- Howell1[Howell1$age >= 18, ]

# Height model
mu_list <- seq(from = 150, to = 160, length.out = 100)
sigma_list <- seq(from = 7, to = 9, length.out = 100)
post <- expand.grid(mu = mu_list, sigma = sigma_list)
post$LL <- sapply(1:nrow(post), 
                  function(i) 
                    sum(dnorm(d2$height, post$mu[[i]], post$sigma[[i]], log = TRUE)))
post$prod <- post$LL +
  dnorm(post$mu, 178, 20, log = TRUE) +
  dunif(post$sigma, 0, 50, log = TRUE)
post$prob <- exp(post$prod - max(post$prod))

# Sample from posterior
sample_rows <- sample(1:nrow(post) - 1, size = 1e4, replace = TRUE, prob = post$prob)
sample_mu_sigma <- post[sample_rows, c("mu", "sigma")]
plot(sample_mu_sigma, cex = 0.5, pch = 16, col = col.alpha(rangi2, 0.1))

```

### Example linear regression (p. 97-110)
```{r example_4.4}
# Scatter plot of height and weight
plot(d2$height, d2$weight)

# Define mean of weight for centering
xbar <- mean(d2$weight)

m4.3 <- quap(
    alist(
        height ~ dnorm(mu, sigma),
        mu <- a + b * (weight - xbar),
        a ~ dnorm(178, 20),
        b ~ dlnorm(0, 1),
        sigma ~ dunif(0, 50)
    ),
    data = d2
)

# Summary of posterior
precis(m4.3)

# Sample from the posterior
post <- extract.samples(m4.3)
# Mean of posterior a and b parameters
a_map <- mean(post$a)
b_map <- mean(post$b)
ggplot(data = d2, aes(x = weight, y = height)) +
  geom_point(color = rangi2, shape = "o", size = 4) +
  geom_abline(slope = b_map, intercept = a_map - b_map * xbar, lwd = 1) +
  geom_abline(data = post[1:20, ],
              aes(slope = b, intercept = a - b * xbar),
              alpha = 0.3) +
  theme_minimal()

# Distribution of mu at weight values
weight_seq <- 25:70
mu <- link(m4.3, data = data.frame(weight = weight_seq))
mu_mean <- apply(mu, 2, mean)
mu_pi <- apply(mu, 2, PI, prob = 0.89)
# Reshape `mu_pi` for easier plotting
mu_pi <- t(mu_pi) %>%
  as_tibble() %>%
  rename_with(~{paste("mu", .x, sep = "_")}) %>%
  mutate(weight = weight_seq)
# Generate posterior predictions
sim_height <- sim(m4.3, data = list(weight = weight_seq), n = 1e4)
height_pi <- apply(sim_height, 2, PI, prob = 0.89) %>%
  t() %>%
  as_tibble() %>%
  rename_with(~{paste("height", .x, sep = "_")}) %>%
  mutate(weight = weight_seq)


ggplot() +
  geom_point(data = d2,
             aes(x = weight, y = height),
             color = rangi2,
             shape = "o",
             size = 4) +
  geom_abline(slope = b_map, intercept = a_map - b_map * xbar) +
  geom_ribbon(data = mu_pi, aes(x = weight, ymin = `mu_5%`, ymax = `mu_94%`),
              alpha = 0.4,
              fill = "gray") +
  geom_ribbon(data = height_pi,
              aes(x = weight, ymin = `height_5%`, ymax = `height_94%`),
              alpha = 0.3) +
  theme_minimal()
```

## Problems
**4E1.** The first line $y_i \sim \text{Normal}(\mu,\sigma)$ is the likelihood.

**4E2.** There are two paramaters in the posterior distribution $\mu$ and $\sigma$.

**4E3.** Bayes' Theorem says
$$
\begin{align*}
\text{Pr}(\mu,\sigma|y_i)&\propto\text{Pr}(y_i|\mu,\sigma)\text{Pr}(\mu)\text{Pr}(\sigma) \\
&=\frac{\text{Normal}(y_i|\mu,\sigma)\text{Normal}(\mu|0,10)\text{Exponential}(\sigma|1)}{\int\text{Normal}(y_i|\mu,\sigma)\text{Normal}(\mu|0,10)\text{Exponential}(\sigma|1)\text d\mu\text d\sigma}
\end{align*}
$$

**4E4.** The linear model is the second line $\mu_i=\alpha + \beta x_i$.

**4E5.** There are three parameters in the posterior distribution: $\alpha$, $\beta$, and $\sigma$.

**4M1.** 
```{r p4m1}
prior_mu <- rnorm(1e4, mean = 0, sd = 10)
prior_s <- rexp(1e4, rate = 1)
y_prior_pred <- rnorm(1e4, mean = prior_mu, s = prior_s)
hist(y_prior_pred)
```

**4M2.**
```{r p4m2, eval=FALSE}
quap(
    alist(
        y ~ dnorm(mu, sigma),
        mu ~ dnorm(0, 10),
        sigma ~ dexp(1)
    )
)
```

**4M3.**
$$
\begin{align*}
y_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta x_i \\
\alpha &\sim \text{Normal}(0, 10) \\
\beta &\sim \text{Uniform}(0, 1) \\
\sigma &\sim \text{Exponential}(1)
\end{align*}
$$

**4M4.**

A model for this problem could be:

$$
\begin{align*}
y_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta x_i \\
\alpha &\sim \text{Normal}(168, 20) \\
\beta &\sim \text{Exponential}(0.5) \\
\sigma &\sim \text{Exponential}(1)
\end{align*}
$$
where $x_i$ is the year of the study (1, 2, or 3) minus 1. 

* $\alpha$ is the average height in the first year of the study. It's prior has an average of roughly 5'5" and a standard deviation of 20 cm. 
* Since we are dealing with students, we may assume that they don't shrink during the study. So, I chose the Exponential distribution with a mean of 2 cm.
* The variance parameter was chosen as recommended in the chapter.

**4M5.**

No, this doesn't change my priors or model as the fact that students get taller is guaranteed by my prior for $\beta$ already.

**4M6.**

Knowing that the variance of heights among students of the same age was never more than 64 cm would make me change my prior on $\sigma$ to $\text{Uniform}(0,64)$. This would allow for the variance to be positive while encoding the upper-bound to be no more than 64.

**4M7.**
```{r p4m7}
# Fit model 4.3 without centered weights
d2 <- Howell1[Howell1$age >= 18, ]
m4.3_alt <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * weight,
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d2
)

# Summary of posterior
precis(m4.3_alt)
vcov(m4.3_alt)

# Sample from the posterior
post_alt <- extract.samples(m4.3_alt)
# Mean of posterior a and b parameters
a_map <- mean(post_alt$a)
b_map <- mean(post_alt$b)
ggplot(data = d2, aes(x = weight, y = height)) +
  geom_point(color = rangi2, shape = "o", size = 4) +
  geom_abline(slope = b_map, intercept = a_map - b_map, lwd = 1) +
  geom_abline(data = post_alt[1:20, ],
              aes(slope = b, intercept = a - b),
              alpha = 0.3) +
  theme_minimal()

# Distribution of mu at weight values
weight_seq <- 25:70
mu <- link(m4.3_alt, data = data.frame(weight = weight_seq))
mu_mean <- apply(mu, 2, mean)
mu_pi <- apply(mu, 2, PI, prob = 0.89)
# Reshape `mu_pi` for easier plotting
mu_pi <- t(mu_pi) %>%
  as_tibble() %>%
  rename_with(~{paste("mu", .x, sep = "_")}) %>%
  mutate(weight = weight_seq)
# Generate posterior predictions
sim_height <- sim(m4.3_alt, data = list(weight = weight_seq), n = 1e4)
height_pi <- apply(sim_height, 2, PI, prob = 0.89) %>%
  t() %>%
  as_tibble() %>%
  rename_with(~{paste("height", .x, sep = "_")}) %>%
  mutate(weight = weight_seq)


ggplot() +
  geom_point(data = d2,
             aes(x = weight, y = height),
             color = rangi2,
             shape = "o",
             size = 4) +
  geom_abline(slope = b_map, intercept = a_map) +
  geom_ribbon(data = mu_pi, aes(x = weight, ymin = `mu_5%`, ymax = `mu_94%`),
              alpha = 0.4,
              fill = "gray") +
  geom_ribbon(data = height_pi,
              aes(x = weight, ymin = `height_5%`, ymax = `height_94%`),
              alpha = 0.3) +
  theme_minimal()
```

Observations:

- The estimates for $\sigma$ don't change; the estimates for $\beta$ are very similar; but, the estimate for $\alpha$ changes: the mean is lower (154.60 to 114.53), but the standard deviation is greater (0.27 vs. 1.90)
- The parameters now have some covariance, which they didn't have when the weights were centered. However, the covariance is relatively small: the largest (in abs. value) is between $\text{Cov}(\alpha, \beta)$ = `r vcov(m4.3_alt)[1, 2]`.
- The predictions of the centered and non-centered models are similar.

**4M8.**
```{r p4m8}
# Load cherry blossoms data from rethinking
data("cherry_blossoms")
# Remove years with missing data
cb <- cherry_blossoms[complete.cases(cherry_blossoms$doy), ]

# Define function to vary output by number of knots and spread of prior on w
vary_inputs <- function(num_knots, w_spread = 10) {
  knot_list <- quantile(cb$year, probs = seq(0, 1, length.out = num_knots))
  
  # Define splines
  B <- splines::bs(cb$year, knots = knot_list[-c(1, num_knots)], degree = 3, 
                   intercept = TRUE)
  # Fit the model
  cb_m1 <- quap(
    alist(
      D ~ dnorm(mu, sigma),
      mu <- a + B %*% w,
      a ~ dnorm(100, 10),
      w ~ dnorm(0, w_spread),
      sigma ~ dexp(1)
    ),
    data = list(D = cb$doy, B = B, w_spread = w_spread),
    start = list(w = rep(0, ncol(B)))
  )
  
  # Extract samples from the posterior and calculate $\mu$ and its 97% interval
  post <- extract.samples(cb_m1)
  w <- apply(post$w, 2, mean)
  mu <- link(cb_m1)
  mu_PI <- apply(mu, 2, PI, 0.97)
  
  # Plots
  plot_basis <- B |>
    as_tibble(.name_repair = ~paste0("B", .x)) |>
    bind_cols("year" = cb$year) |>
    pivot_longer(cols = -year) |>
    ggplot() +
    geom_line(aes(x = year, y = value, group = name)) +
    scale_y_continuous(breaks = c(0, 1), name = "basis value") +
    scale_x_continuous(breaks = seq(800, 2000, by = 200)) +
    geom_text(data = tibble(x = knot_list, y = 1.05, label = "+"),
              aes(x = x, y = y, label = label),
              size = 6) + 
    theme_minimal()
  
  plot_wtd_basis <- B %*% diag(w) |>
    as_tibble() |>
    bind_cols("year" = cb$year) |>
    pivot_longer(cols = -year) |>
    ggplot() +
    geom_line(aes(x = year, y = value, group = name)) +
    scale_y_continuous(breaks = c(0), name = "basis * weight") +
    scale_x_continuous(breaks = seq(800, 2000, by = 200)) +
    geom_text(data = tibble(x = knot_list, y = 6.05, label = "+"),
              aes(x = x, y = y, label = label),
              size = 6) +
    theme_minimal()
  
  mu_pi <- t(mu_PI) |>
    as_tibble() |>
    bind_cols("year" = cb$year)
  plot_mu <- ggplot() +
    geom_point(data = cb,
               aes(x = year, y = doy),
               color = rangi2,
               alpha = 0.8,
               pch = 16) +
    geom_ribbon(data = mu_pi,
                aes(x = year, ymin = `2%`, ymax = `98%`),
                alpha = 0.5) +
    scale_x_continuous(breaks = seq(800, 2000, by = 200),
                       name = "year") +
    labs(y = "Day in year") +
    theme_minimal()
  
  return(gridExtra::grid.arrange(plot_basis, plot_wtd_basis, plot_mu, nrow = 3))
}
vary_inputs(40, 25)
```

**4H1.**
```{r p4h1}
# Define the new weights
new_weight <- c(46.95, 43.72, 64.78, 32.59, 54.63)
# Simulate posterior predictions for new weights
new_height <- sim(m4.3, data = list(weight = new_weight))
new_height_pi <- apply(new_height, 2, PI, prob = 0.89)
new_height_pi |>
  as_tibble(.name_repair = "unique") |>
  summarize(across(everything(),
            list(mean = mean, lower = min, upper = max))) |>
  pivot_longer(everything()) |>
  separate(name, into = c("var", "measure"), sep = "_") |>
  pivot_wider(id_cols = var, names_from = measure) |>
  mutate(weight = new_weight) |>
  select(individual = var, weight, mean, lower, upper) |>
  gt(rowname_col = "individual") |>
    tab_stubhead(label = "Individual") |>
    cols_label(
      weight = "weight",
      mean = "expected height",
      lower = "lower bound",
      upper = "upper bound"
    ) |>
  tab_spanner(
    label = "89% interval",
    columns = c(lower, upper)
  )
```

**4H2.**

(a) Fitting a linear regression to the Howell data with ages below 18
```{r p4h2a}
d3 <- Howell1[Howell1$age < 18, ]
# Scatter plot of height and weight
plot(d3$weight, d3$height)

# Define mean of weight for centering
xbar <- mean(d3$weight)

m_l_u18 <- quap(
    alist(
        height ~ dnorm(mu, sigma),
        mu <- a + b * (weight - xbar),
        a ~ dnorm(120, 20),
        b ~ dlnorm(0, 1),
        sigma ~ dunif(0, 50)
    ),
    data = d3
)

# Summary of posterior
precis(m_l_u18)
```
The model predicts that for every 10 units of increase in weight, the child will be about 27cm taller.

(b) Plotting the raw data and model summaries

```{r ph2b}
# Sample from the posterior
post <- extract.samples(m_l_u18)
# Mean of posterior a and b parameters
a_map <- mean(post$a)
b_map <- mean(post$b)
# Distribution of mu at weight values
weight_seq <- d3$weight
mu <- link(m_l_u18, data = data.frame(weight = weight_seq))
mu_mean <- apply(mu, 2, mean)
mu_pi <- apply(mu, 2, PI, prob = 0.89)
# Reshape `mu_pi` for easier plotting
mu_pi <- t(mu_pi) %>%
  as_tibble() %>%
  rename_with(~{paste("mu", .x, sep = "_")}) %>%
  mutate(weight = weight_seq)
# Generate posterior predictions
sim_height <- sim(m_l_u18, data = list(weight = weight_seq), n = 1e4)
height_pi <- apply(sim_height, 2, PI, prob = 0.89) %>%
  t() %>%
  as_tibble() %>%
  rename_with(~{paste("height", .x, sep = "_")}) %>%
  mutate(weight = weight_seq)


ggplot() +
  geom_point(data = d3,
             aes(x = weight, y = height),
             color = rangi2,
             shape = "o",
             size = 4) +
  geom_abline(slope = b_map, intercept = a_map - b_map * xbar) +
  geom_ribbon(data = mu_pi, aes(x = weight, ymin = `mu_5%`, ymax = `mu_94%`),
              alpha = 0.4,
              fill = "gray") +
  geom_ribbon(data = height_pi,
              aes(x = weight, ymin = `height_5%`, ymax = `height_94%`),
              alpha = 0.3) +
  theme_minimal()
```

(c) The model does not fit very well at the lower and upper ends of the weight values. The reason is that the data appear to be non-linear. One could try a transformation of weight, say the logarithm or a root, so that the relationship with height is closer to linear.

**4H3.**

(a) Log-linear model for height as a function of weight with all the Howell data
```{r p4h3a}
m_howell <- quap(
    alist(
        height ~ dnorm(mu, sigma),
        mu <- a + b * log(weight),
        a ~ dnorm(100, 20),
        b ~ dlnorm(0, 1),
        sigma ~ dunif(0, 50)
    ),
    data = Howell1
)

# Summary of posterior
precis(m_howell)

# Sample from the posterior
post <- extract.samples(m_howell)
# Mean of posterior a and b parameters
a_map <- mean(post$a)
b_map <- mean(post$b)
# Distribution of mu at weight values
weight_seq <- Howell1$weight
mu <- link(m_howell, data = data.frame(weight = weight_seq))
mu_mean <- apply(mu, 2, mean)
mu_pi <- apply(mu, 2, PI, prob = 0.97)
# Reshape `mu_pi` for easier plotting
mu_pi <- t(mu_pi) %>%
  as_tibble() %>%
  rename_with(~{paste("mu", .x, sep = "_")}) %>%
  mutate(weight = weight_seq)
# Generate posterior predictions
sim_height <- sim(m_howell, data = list(weight = weight_seq), n = 1e4)
height_pi <- apply(sim_height, 2, PI, prob = 0.97) %>%
  t() %>%
  as_tibble() %>%
  rename_with(~{paste("height", .x, sep = "_")}) %>%
  mutate(weight = weight_seq)
```

The parameter estimates of the model are difficult to interpret due to the `log(weight)` transformation. For $b$ we can say that $p$% increase in weight would cause the model to predict (on average) a $b\log(1+p/100)$ increase in height. This is a bit awkward, but for $a% it's worse. One way to say it would be that it is the average height (predicted by the model) when the weight is 1. But, $a$ is negative, which doesn't make sense. But, as far as I know, there are no post-birth humans that weight one kg. 

(b) Plotting model estimates

```{r p4h3b}
mean_function <- function(xvar) {
  a_map + b_map * log(xvar)
}
ggplot() +
  geom_point(data = Howell1,
             aes(x = weight, y = height),
             color = rangi2,
             shape = "o",
             size = 4) +
  stat_function(fun = mean_function) +
  geom_ribbon(data = mu_pi, aes(x = weight, ymin = `mu_2%`, ymax = `mu_98%`),
              alpha = 0.4,
              fill = "gray") +
  geom_ribbon(data = height_pi,
              aes(x = weight, ymin = `height_2%`, ymax = `height_98%`),
              alpha = 0.3) +
  theme_minimal()
```

**4H4.**
```{r p4h4}
# Generate `n` combinations of parameters from the priors
set.seed(1622)
n <- 100
a <- rnorm(n, 150, 50)
b1 <- rlnorm(n, 0, 1)
b2 <- rnorm(n, -2, 5)

# Generate plotting data for each of the combinations of the parameters
weight_s <- (Howell1$weight - mean(Howell1$weight)) / sd(Howell1$weight)
xs <- seq(from = min(weight_s), to = max(weight_s), length.out = 100)
plot_data <- tibble(
  parameter_idx = sort(rep(1:n, n)),
  x = rep(xs, n),
  y = a[parameter_idx] + b1[parameter_idx] * x + b2[parameter_idx] * x^2
)

# Plot prior simulations
plot_data |>
  ggplot(aes(x = x, y = y, group = parameter_idx)) +
  geom_line(alpha = 0.3, color = rangi2) +
  # guidelines for extremes of human heights
  geom_hline(yintercept = 0) +
  geom_hline(yintercept = 272)
```
The above parameters generate prior estimates in the normal range. The $\alpha$ parameter's mean seems to shift the cloud of curves up and down, while the sd shrinks or expands the range. The $\beta_1$ parameter seems to set a "steepness" of the curves from left to right, while the $\beta_2$ parameter controls the "curviness".

**4H5.**
```{r}
# Load and plot the data
data(cherry_blossoms)

cherry_blossoms |>
  ggplot(aes(x = temp, y = doy)) +
  geom_point(alpha = 0.3)
```

Let's use a linear model to begin.
```{r}
# Remove observations that are missing `doy` or `temp`
d <- cherry_blossoms |>
  filter(!is.na(doy), !is.na(temp))
mean_temp <- mean(d$temp)

cherry_mdl_l <- quap(
  alist(
    doy ~ dnorm(mu, sigma),
    mu <- a + b * (temp - mean_temp),
    a ~ dnorm(105, 40),
    b ~ dnorm(5, 1),
    sigma ~ dexp(1)
  ),
  data = d
)

precis(cherry_mdl_l)

# Sample from the posterior
post <- extract.samples(cherry_mdl_l)
# Mean of posterior a and b parameters
a_map <- mean(post$a)
b_map <- mean(post$b)
# Distribution of mu at temp values
mu <- link(cherry_mdl_l, data = data.frame(temp = d$temp))
mu_mean <- apply(mu, 2, mean)
mu_pi <- apply(mu, 2, PI, prob = 0.89)
# Reshape `mu_pi` for easier plotting
mu_pi <- t(mu_pi) %>%
  as_tibble() %>%
  rename_with(~{paste("mu", .x, sep = "_")}) %>%
  mutate(temp = d$temp)
# Generate posterior predictions
sim_doy <- sim(cherry_mdl_l, data = list(temp = d$temp), n = 1e4)
doy_pi <- apply(sim_doy, 2, PI, prob = 0.89) %>%
  t() %>%
  as_tibble() %>%
  rename_with(~{paste("doy", .x, sep = "_")}) %>%
  mutate(temp = d$temp)

ggplot() +
  geom_point(data = d,
             aes(x = temp, y = doy),
             color = rangi2,
             shape = "o",
             size = 4) +
  geom_abline(slope = b_map, intercept = a_map - b_map * mean_temp) +
  geom_ribbon(data = mu_pi, aes(x = temp, ymin = `mu_5%`, ymax = `mu_94%`),
              alpha = 0.4,
              fill = "gray") +
  geom_ribbon(data = doy_pi,
              aes(x = temp, ymin = `doy_5%`, ymax = `doy_94%`),
              alpha = 0.3) +
  theme_minimal()
```

While I didn't try a polynomial or spline, the linear model seems like it works. The training data falls pretty well within the 89% prediction interval, the apparent inverse relationship between March temperature and blossom day is captured. However, the predition interval is pretty wide (about 20 days at any given March temperature), which may not be suitable for some applications.

**4H6.**
```{r p4h6}
# Remove rows with missing first blossom days
cb <- cherry_blossoms[complete.cases(cherry_blossoms$doy), ]
# Define knots
knot_list <- quantile(cb$year, probs = seq(0, 1, length.out = 15))
# Define splines
n <- 100
yrs <- floor(seq(from = min(cb$year), to = max(cb$year), length.out = n))
basis <- splines::bs(yrs, knots = knot_list[-c(1, 15)], degree = 3,
                  intercept = TRUE)

# Generate sample parameters from the prior
a <- rnorm(n, 100, 10)
w <- mvtnorm::rmvnorm(n, sigma = diag(20, ncol(basis)))
# Create plotting data for the prior
plot_data <- basis %*% t(w) |>
  as_tibble(..name_repair =  "unique") |>
  pivot_longer(everything(), values_to = "basis_wt") |>
  arrange(name) |>
  mutate(parameter_idx = as.integer(str_extract(name, "[0-9]+")),
         year = rep(yrs, n),
         a = a[parameter_idx],
         sim_doy = a + basis_wt)
plot_data |>
  ggplot(aes(x = year, y = sim_doy, group = name)) +
  geom_line(alpha = 0.3, color = rangi2) +
  theme(legend.position = "none")
```

I changed the value of the standard deviation in the prior for $w$. The values I tried were 1, 10 (the value in the book), and 20. The changes didn't affect the range of possible values (roughly 70 - 130) or that values in the middle of the range (say, 90 - 110) were more likely. However, the curves were flatter with the value 1 than with 10 and "wigglier" with 20. This makes sense as weights closer to zero (more likely in a standard normal) will dampen the range of values of the basis functions, while larger weights (more likely with a standard deviation of 20) will increase the movement of the basis functions.

**4H8.**
```{r p4h8}
# Define knots
knot_list <- quantile(cb$year, probs = seq(0, 1, length.out = 15))
# Define splines
b <- splines::bs(cb$year, knots = knot_list[-c(1, 15)], degree = 3,
                  intercept = TRUE)
# Fit original model
cb_mdl_orig <- quap(
  alist(
    d ~ dnorm(mu, sigma),
    mu <- a + b %*% w,
    a ~ dnorm(100, 10),
    w ~ dnorm(0, 10),
    sigma ~ dexp(1)
  ),
  data = list(d = cb$doy, b = b),
  start = list(w = rep(0, ncol(b)))
)
precis(cb_mdl_orig, depth = 2)
mu_orig <- link(cb_mdl_orig)
mu_orig_pi <- t(apply(mu_orig, 2, PI, 0.97)) |>
  as_tibble() |>
  rename_with(\(x) paste("mu", x, sep = "_")) |>
  mutate(year = cb$year)
ggplot() +
  geom_point(data = cb,
             aes(x = year, y = doy),
             color = rangi2,
             shape = "o",
             size = 4) +
  geom_ribbon(data = mu_orig_pi,
              aes(x = year, ymin = `mu_2%`, ymax = `mu_98%`),
              alpha = 0.5,
              fill = "black") +
  labs(title = "Original Cherry Blossom Model Posterior for mu")

# Fit model without intercept
cb_mdl_noint <- quap(
  alist(
    d ~ dnorm(mu, sigma),
    mu <- b %*% w,
    w ~ dnorm(100, 10),
    sigma ~ dexp(1)
  ),
  data = list(d = cb$doy, b = b),
  start = list(w = rep(0, ncol(b)))
)
mu_noint <- link(cb_mdl_noint)
mu_noint_pi <- t(apply(mu_noint, 2, PI, 0.97)) |>
  as_tibble() |>
  rename_with(\(x) paste("mu", x, sep = "_")) |>
  mutate(year = cb$year,
         lower = `mu_2%`,
         upper = `mu_98%`)
ggplot() +
  geom_point(data = cb,
             aes(x = year, y = doy),
             color = rangi2,
             shape = "o",
             size = 4) +
  geom_ribbon(data = mu_noint_pi,
              aes(x = year, ymin = lower, ymax = upper),
              alpha = 0.5,
              fill = "black") +
  labs(title = "Cherry Blossom Model with No Intercept Posterior for mu")
```

My original solution to this was to keep the same priors from the original model, but modify the data so that `d == cb$doy - mean(cb$doy)`. In essence, the model was of the deviations from the sample mean. Then, I could add the sample mean back to get a prediction of the actual day of the year. However, I thought this was a little janky so I got some help from [Jake Thompson](https://sr2-solutions.wjakethompson.com/linear-models-causal-inference.html). I like this solution better.


